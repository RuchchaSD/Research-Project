\section{Methodology}
\label{sec:methodology}

Cache‑miss–induced CPU stalls are the leading performance bottleneck in sparse bundle adjustment.
We address this by: (1) placing all frequently‑accessed structures in contiguous memory blocks
(\S\ref{subsec:parameter_ordering}), (2) storing data associations in flat arrays for efficient memory access
(\S\ref{subsec:connectivity_storage}), (3) accelerating the Schur complement computation using a novel 
two‑stage pipeline architecture (\S\ref{subsec:t2sc}), and (4) leveraging highly‑optimized Level‑3 BLAS 
kernels for dense linear algebra operations (\S\ref{subsec:blas_apis}). These optimizations are integrated 
into the canonical sparse bundle adjustment solver of 
Lourakis \textit{et al.},\cite{lourakis2004design,lourakis2009sba}, which we parallelize using data‑level 
concurrency as described in Section~3.

\subsection{Special Parameter \& Edge Ordering}
\label{subsec:parameter_ordering}

\textbf{Dense parameter layout and edge bucketing.} To minimize cache misses we transform the
input state into a cache‑coherent memory layout through two key steps. First, we assign compact
zero‑based IDs to poses and landmarks and pack their parameters contiguously (6or9 elements per
pose, 3 per landmark). The ordering translates directly to the global Hessian $\mathbf H$ and right‑hand
side $\mathbf b$, giving stride‑1 access in every kernel. Second, we bucket edges in a
\emph{landmark‑first} hierarchy—each landmark stores a consecutive list of the poses that observe it.
During Jacobian assembly a pose therefore touches its connected landmarks sequentially, preventing
pointer chasing. Intermediate Schur blocks are addressed via closed‑form index math to keep cache
lines hot throughout matrix multiplication.

\begin{figure}[t]
\centering
\includegraphics[width=0.75\linewidth]{figs/placeholder}
\caption{Edge bucketing layout showing landmark‑first organization with sequential pose/landmark IDs.}
\label{fig:dense_layout}
\end{figure}

\textbf{Forward link} $\rightarrow$ The dense IDs serve as direct indices for the flat pattern maps of
§\ref{subsec:connectivity_storage}.

\subsection{Sparse Graph Connectivity‑Pattern Storage}
\label{subsec:connectivity_storage}

\textbf{STL‑free flat‑array architecture.} Building on the IDs above we replace pointer‑heavy STL
containers with five monotonic arrays that encode the bipartite graph:

\begin{lstlisting}[language=C++, basicstyle=\ttfamily\small]
edge_v1[|E|]        // Edge -> Pose
edge_v2[|E|]        // Edge -> Landmark
v1_v2Edge[|V_p|]   // Pose  -> {V_l, E} list
v2_v1Edge[|V_l|]   // Landm -> {V_p, E} list
v1_edge_pairs[idx] // Pose-pose co-obs list
\end{lstlisting}

with the closed‑form mapping
\begin{equation}
\text{idx}(r,c)=r(r-1)/2+c, .
\end{equation}
Every matrix block—Jacobian, Hessian, Schur—can therefore be located in $\mathcal O(1)$ time
without hash tables.

\textbf{Closed-form block indexing across all matrices.} The dense IDs enable direct mathematical indexing 
into multiple matrix structures without hash table lookups. The flat array architecture supports $O(1)$ 
block access across all major dense data structures mentioned in \S\ref{subsec:parameter_ordering}.


\hspace{-1.5em}
\textbf{Example: Accessing Hessian blocks} \newline
    For pose $i$, the $\mathbf{A}$ diagonal block is accessed as:
\begin{lstlisting}[language=C++, basicstyle=\ttfamily\small]
hes_A.block(0, i * vertex1Size, 
                     vertex1Size, vertex1Size)
\end{lstlisting}

This uniform indexing pattern applies to all matrix operations: $\mathbf{B}$ landmark blocks, 
marginalization matrix $\mathbf{Y}$, parameter vectors, and gradient assembly. The closed-form index 
math enables direct $O(1)$ access to any block in the Hessian or Schur complement matrices without 
pointer chasing.

\textbf{Cache-friendly data layout.} Each array stores homogeneous data types (\texttt{uint8\_t} for poses, \texttt{uint16\_t} 
for landmarks, \texttt{uint32\_t} for edges) in contiguous memory, enabling efficient vectorized access patterns during 
parallel Hessian assembly. The landmark-first edge organization from \S\ref{subsec:parameter_ordering} ensures 
that \texttt{v1\_v2Edge} traversals access landmarks sequentially, maximizing cache line utilization during 
the T$^2$SC tiling phase (\S\ref{subsec:t2sc}).

\textbf{Benefits.} The flat array architecture achieves $O(1)$ block access through simple index 
computation versus multiple pointer dereferences in g2o's CSR format. The contiguous layout enables 
efficient OpenMP parallelization and direct zero-copy handoff to Level-3 BLAS routines (\S\ref{subsec:blas_apis}), 
while reducing memory overhead and improving cache locality for the T$^2$SC pipeline (\S\ref{subsec:t2sc}).

\begin{figure}[t]
\centering
\includegraphics[width=0.75\linewidth]{figs/placeholder}
\caption{Left: g2o CSR storage. Right: proposed flat‑array encoding with $\mathcal O(1)$ block lookup.}
\label{fig:connectivity_storage}
\end{figure}

\textbf{Forward link} $\rightarrow$ The contiguous edge lists become the work‑queue for the T$^2$SC
tiles (§\ref{subsec:t2sc}).

\subsection{T$^2$SC --- Tiled, Two-Phase Schur Complement}
\label{subsec:t2sc}

\textbf{Pipeline motivation.} Schur complement construction—${\mathbf H}_{act}=\mathbf A-\mathbf Y\mathbf W^\top$—dominates run‑time.
Fetching scattered $\mathbf Y$ and $\mathbf W$ blocks thrashes caches. T$^{2}$SC sidesteps this by
(\emph{i}) copying blocks into pre‑allocated contiguous tiles and (\emph{ii}) feeding those tiles to
batched GEMM.

\textbf{Tile book‑keeping.} At start‑up we allocate a tile per pose pair $(r,c)$ sized from the
co‑observation count available via the flat arrays. Tiles live in a NUMA‑aware arena so all threads
share a single logical address space.

\textbf{Phase I — block staging.} Producer threads copy each required $\mathbf Y$ block once per
LM iteration into its tile; $\mathbf W$ is constant and staged only at the first iteration. The copy
walks the landmark‑first edge lists sequentially so the memory system observes efficient
prefetching.

\textbf{Phase II — GEMM consumption.} Consumer threads dequeue ready tiles and compute
$\mathbf Y \cdot \mathbf W^{\top}$ through Level‑3 BLAS. Diagonal tiles are emitted first so the
pose‑pose factorization can overlap with the outer‑product of off‑diagonals.

\textbf{Measured impact.} On a 16‑core Intel i9‑9900K the pipeline reduces Schur build time from
\SI{50}{\milli\second} to \SI{3.9}{\milli\second} on the smallest BAL scene 
($V_p=49$, $V_l=7776$, $E=31843$) (12.9$\times$). Across the five
datasets the end‑to‑end solver is 3.6,× faster than g2o when limited to ten iterations—the ORB‑SLAM
real‑time budget. On the bandwidth‑constrained quad‑core Cortex‑A53 we still obtain 1.5,× overall
speed‑up, and T$^{2}$SC alone accounts for a consistent 2.5,× cut in Schur time.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.75\linewidth]{figs/placeholder}
  \caption{T$^2$SC pipeline implementation: In producer stage, we copy the $\mathbf{Y}$ blocks
  into contiguous tiles, and in consumer stage, we compute the $\mathbf{Y} \cdot \mathbf{W}^T$ products
  using dense BLAS operations.}
  \label{fig:t2sc_pipeline}
\end{figure}

\textbf{Forward link} $\rightarrow$ Tiles are handed to vendor BLAS libraries as detailed in
§\ref{subsec:blas_apis}.

\subsection{Exposing Matrix Operations via Standard BLAS APIs}
\label{subsec:blas_apis}

\textbf{BLAS‑centric design.} All heavy kernels reduce to Level‑3 routines, allowing us to link
against OpenBLAS, Intel MKL, Vitis BLAS or Arm Performance Libraries unmodified.

\textbf{Unified GEMM interface.} The T²SC pipeline (§\ref{subsec:t2sc}) feeds contiguous tiles 
into a unified batched GEMM interface that abstracts the underlying BLAS calls. Each batch operation 
processes multiple $\mathbf{Y} \cdot \mathbf{W}^T$ products using identical matrix dimensions, 
enabling efficient vectorization and cache reuse across tiles.


\begin{table}[h]
\centering
\caption{Bundle‑adjustment operation $\rightarrow$ BLAS mapping}
\label{tab:blas_mapping}
\begin{tabular}{lll}
\toprule
\textbf{BA Operation} & \textbf{BLAS Kernel} & \textbf{Source blocks} \\
\midrule
$\mathbf J^{\top}\mathbf J$                & \texttt{GEMM\_BATCH} & Contiguous Jacobians \\
$\mathbf J^{\top}\mathbf r$                & \texttt{GEMM}        & Sequential residuals \\
$\mathbf Y\mathbf W^{\top}$                & \texttt{GEMM\_BATCH} & T$^{2}$SC tiles     \\
Landmark solve $\mathbf B^{-1}$                & \texttt{POTRF/POTRS} & Dense blocks        \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Floating point internal operations.} Re‑building the solver in \texttt{float32} cuts memory footprint by
\SI{40}{\percent}. On the BAL suite the $\chi^{2}$ at iteration 5 differs by <0.01 %, confirming that
single precision suffices for real‑time SLAM windows while doubling the working‑set capacity on the
Kria board.

\textbf{Additional numerical optimizations:}
\begin{itemize}
\item \textbf{Mixed precision Jacobian computation:} We use \texttt{double} precision for finite difference calculations to maintain numerical accuracy, then cast results to \texttt{float32} for storage and subsequent operations. This balances accuracy with memory efficiency.
\item \textbf{Parameter scaling:} Following Ceres\cite{agarwal2012ceres} Solver's approach, we implement Jacobi scaling with $s_i = 1/\sqrt{||\mathbf{J}_{\text{col}_i}||^2 + \epsilon}$ to improve numerical conditioning by normalizing parameter magnitudes across different scales (poses vs. landmarks).
\end{itemize}

\textbf{Section summary.} The pipeline of (1) dense parameter layout, (2) flat connectivity arrays, (3)
T$^{2}$SC tiling, and (4) BLAS offload transforms sparse BA into cache‑friendly dense GEMMs. The design
achieves 1.5–3.6,× lower wall‑time than g2o within the 10‑iteration budget on ARM and x86 while
maintaining accuracy parity with Ceres.

