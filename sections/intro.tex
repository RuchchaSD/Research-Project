\section{Introduction}\label{sec:intro}


\begin{figure}[t]
  \centering
  \includegraphics[width=0.9\linewidth]{figs/placeholder}
  \caption{System pipeline overview (placeholder).}
  \label{fig:intro_pipeline}
\end{figure}

\IEEEPARstart{V}{isual} SLAM and structure-from-motion (SfM) are fundamental technologies in robotics, 
mixed reality, and autonomous navigation that reconstruct camera motion and 3D structure from image 
sequences~\cite{cadena2016past,hartley2003multiple}. These systems consist of three main components: 
a front-end that handles visual odometry and feature tracking, a back-end that performs optimization through 
bundle adjustment (BA), and a loop closing module that detects revisited places and corrects accumulated 
drift~\cite{campos2021orb}.

Recent advances have largely solved front-end performance challenges. Modern feature extractors like ORB 
running on SIMD~\cite{rublee2011orb}, FPGA-accelerated variants achieving over 1,200 FPS at just 
\SI{2.1}{\watt}~\cite{fang2017fpga}, and efficient CNN-based detectors like MobileSP running in \SI{4}{ms} 
on embedded GPUs~\cite{liu2022mobilesp} now deliver real-time performance even on edge devices. 

This progress has shifted the computational bottleneck to the BA back-end, where camera poses and 3D landmarks 
are jointly refined through nonlinear optimization. On embedded platforms ranging from quad-core Cortex-A53 to 
automotive-grade octa-core SoCs, BA now consumes \SIrange{55}{70}{\percent} of total SLAM runtime, even with 
modest sliding windows of 10-15 keyframes~\cite{semenova2022quantitative}. The key challenge lies not in raw 
computational intensity, but rather in inefficient memory access patterns. Specifically, the pointer-heavy sparse 
data structures and naive sparse matrix implementations used in existing frameworks lead to frequent cache misses 
and poor data locality. This results in hundreds of milliseconds of stalled CPU pipeline time~\cite{qin2019pi}.

\paragraph*{Acceleration attempts} Prior work has explored three main approaches:

\textbf{(i) GPUs} While solutions like PBA~\cite{wu2011multicore} and MegBA~\cite{ren2022megba} achieve impressive 
500~MFLOPS/W efficiency on desktop GPUs~\cite{hansch2016modern}, their power requirements exceed mobile robot constraints.

\textbf{(ii) Dedicated hardware} FPGA/ASIC accelerators including $\pi$‑BA~\cite{qin2019pi}, BAX~\cite{sun2020bax}, 
BOHA~\cite{liu2023boha} and Archytas~\cite{liu2021archytas} enable millisecond-level Schur complement computation. 
However, they demand substantial on-chip memory ($\pi$‑BA uses 141.5 BRAMs) or impose strict limits on problem size 
(BAX: 16 poses, 256 landmarks maximum).

\textbf{(iii) Multicore CPUs} Modern solvers like \textsc{g2o}~\cite{kummerle2011g} and \textsc{Ceres}~\cite{agarwal2012ceres} 
employ CSR/CSC storage with parallel processing libraries. While effective on desktop CPUs, their pointer-heavy design 
causes frequent cache misses on embedded systems. Simpler alternatives like SBA/Levmar~\cite{lourakis2009sba} run 
$\approx2\times$ slower than \textsc{Ceres} and face similar memory access inefficiencies.

\textbf{Key Insight:} These observations motivate a fundamental rethink: Can we transform sparse BA into cache-friendly 
dense operations while maintaining the flexibility of CPU-based execution? Our approach achieves this by converting 
irregular memory accesses into predictable, BLAS-optimized matrix operations.

We present a cache-optimized BA solver designed from scratch for multicore embedded and desktop CPUs that transforms sparse BA into 
batched dense GEMMs through four key innovations: (1) \emph{dense parameter layout and landmark-first edge bucketing} 
that eliminates pointer chasing, (2) \emph{flat connectivity arrays} enabling $\mathcal{O}(1)$ block access without 
hash tables, (3) a \emph{tiled two-phase Schur complement} (T$^{2}$SC) that overlaps memory staging with batched BLAS, 
and (4) a \emph{unified BLAS interface} that maps all heavy kernels to Level-3 routines for vendor library compatibility.

Our main contributions are:
\begin{itemize}[leftmargin=*]\setlength\itemsep{2pt}
\item A dense local-ID scheme and landmark-first edge bucketing that pack parameters contiguously and eliminate 
pointer chasing during Jacobian assembly (\S\ref{subsec:parameter_ordering});
\item Flat connectivity arrays with closed-form indexing enabling $\mathcal{O}(1)$ 
block lookup across all matrix structures without STL containers (\S\ref{subsec:connectivity_storage});
\item \textbf{T$^{2}$SC}: a producer-consumer tiled Schur pipeline that stages $\mathbf{Y}$ and $\mathbf{W}$ blocks 
into contiguous tiles for batched GEMM, reducing Schur build time by up to 12.9$\times$ (\S\ref{subsec:t2sc});
\item Complete mapping of BA operations ($\mathbf{J}^{\top}\mathbf{J}$, $\mathbf{Y}\mathbf{W}^{\top}$, landmark solves) 
to Level-3 BLAS kernels, enabling drop-in use of OpenBLAS, MKL, or Arm Performance Libraries (\S\ref{subsec:blas_apis});
\item Comprehensive evaluation on BAL datasets showing 1.2--2.7$\times$ speedup on Cortex-A53 and 2.3--8.8$\times$ 
on Core i9-9900K while maintaining accuracy within 0.52\% of optimal convergence (\S\ref{sec:results}).
\end{itemize}
